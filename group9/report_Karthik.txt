I normalized the raw counts so that every cell sums to the median counts in the data. The normalized counts were then log-transformed after the addition of one. Using a Negative-Binomial model for the null variability in the data, I identified a set of 415 highly variable genes in the training data. The log-transformed normalized values of these genes alone were used as features for a random forest model (see below). Additionally for each cell, I included the total number of genes detected, the total number of transcripts detected and the Shannon entropy of expression (computed as -\sum{p(x)*log(p(x))}, where p(x) is the normalized expression of gene x) as 3 additional features. Next I trained a RF classifier on 70% of the training data, and tested it on the remaining 30% held out data. The classification accuracy was 10% and the AUC was 0.91. The samples were ranked based on the # of trees (probability) that voted as "singlet" or "doublet" in order to compute the AUC. 

The same normalization procedure was followed for the test data, which was then supplied to the RF classifier to predict class labels and class probabilities. These were compiled for the  final submission.
